from transformers import pipeline

qa = pipeline("question-answering", model="deepset/roberta-base-squad2")

context = "Jean Dupont a travaill√© chez Airbus pendant 5 ans comme ing√©nieur en intelligence artificielle."

question = "O√π Jean Dupont a-t-il travaill√© ?"

result = qa(question=question, context=context)

print(result["answer"])



import streamlit as st
from transformers import pipeline
from sentence_transformers import SentenceTransformer, util
import torch

# Masquer le menu de navigation standard de Streamlit (pages)
hide_default_menu = """
    <style>
    [data-testid="stSidebarNav"] {
        display: none;
    }
    </style>
"""
st.markdown(hide_default_menu, unsafe_allow_html=True)


# --- Chargement des mod√®les ---
qa_pipeline = pipeline("question-answering", model="distilbert-base-uncased-distilled-squad")
embedder = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')

# --- Charger et d√©couper le fichier profil.txt ---
def charger_profil(fichier):
    with open(fichier, 'r', encoding='utf-8') as f:
        texte = f.read()
    # D√©couper le texte en blocs pertinents
    blocs = [bloc.strip() for bloc in texte.split("\n\n") if bloc.strip()]
    return blocs

# --- Trouver le bloc le plus pertinent ---
def bloc_pertinent(question, blocs):
    embeddings_blocs = embedder.encode(blocs, convert_to_tensor=True)
    embedding_question = embedder.encode(question, convert_to_tensor=True)
    scores = util.cos_sim(embedding_question, embeddings_blocs)
    meilleur_index = torch.argmax(scores)
    return blocs[meilleur_index]

# --- R√©pondre √† une question ---
def repondre(question, contexte):
    try:
        result = qa_pipeline(question=question, context=contexte)
        return result["answer"]
    except:
        return "D√©sol√©, je n‚Äôai pas pu r√©pondre √† cette question."

# --- Gestion manuelle de questions simples ---
def reponse_personnalisee(question):
    q = question.lower()

    if any(kw in q for kw in ["bonjour", "salut", "hello", "bonsoir"]):
        return "Bonjour et bienvenue ! Je suis IAMBA, l‚Äôagent IA de Yamba Ars√®ne Goubgou. Pose-moi toutes tes questions sur son parcours, ses comp√©tences, ses projets ou son exp√©rience. üòä"
    
    elif any(kw in q for kw in ["merci", "thanks"]):
        return "Avec plaisir ! Si tu veux en savoir plus, je suis toujours disponible."
    
    elif any(kw in q for kw in ["qui es-tu", "pr√©sente-toi", "tu es qui", "c'est quoi iamba"]):
        return ("Je suis IAMBA, un agent conversationnel intelligent con√ßu pour repr√©senter "
                "Yamba Ars√®ne Goubgou. Je peux t‚Äôaider √† d√©couvrir son parcours acad√©mique, "
                "son exp√©rience professionnelle, ses comp√©tences techniques et humaines, "
                "ainsi que ses distinctions et projets.")
    
    elif any(kw in q for kw in ["√ßa va", "tu vas bien", "comment tu vas"]):
        return "Je vais tr√®s bien, merci ! Et toi ? Je suis pr√™t √† r√©pondre √† toutes tes questions. üòÑ"

    elif any(kw in q for kw in ["contact", "joindre", "email", "t√©l√©phone"]):
        return ("Tu peux contacter Yamba par email √† **lamoussama225@gmail.com** ou par t√©l√©phone au **+33 7 53 60 73 32**. "
                "Tu peux aussi consulter ses projets techniques sur GitHub : [github.com/ygoubgou](https://github.com/ygoubgou).")

    return None  # Si rien de d√©tect√©

# --- Interface Streamlit ---
st.set_page_config(page_title="Chatbot IA - IAMBA", page_icon="ü§ñ")
st.title("ü§ñ Agent IA - IAMBA")

# Chargement du profil
blocs_profil = charger_profil("profil.txt")

# Champ de saisie utilisateur
question = st.text_input("Pose ta question √† IAMBA (ex: Formation, Comp√©tences, Exp√©riences...)")

if question:
    # V√©rifie s‚Äôil y a une r√©ponse personnalis√©e
    reponse_man = reponse_personnalisee(question)
    if reponse_man:
        st.markdown("**üß† R√©ponse :**")
        st.success(reponse_man)
    else:
        contexte = bloc_pertinent(question, blocs_profil)
        reponse = repondre(question, contexte)
        
        st.markdown("**üìÑ Contexte utilis√© :**")
        st.info(contexte)
        
        st.markdown("**üß† R√©ponse :**")
        st.success(reponse)



import requests

API_URL = "https://api-inference.huggingface.co/models/deepset/roberta-base-squad2"
headers = {"Authorization": "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"}  # Ton token Hugging Face

def question_reponse(question, contexte):
    payload = {
        "inputs": {
            "question": question,
            "context": contexte
        }
    }
    response = requests.post(API_URL, headers=headers, json=payload)
    return response.json()[0]["answer"]










# Bon dans AI.PY
import streamlit as st
import requests
from sentence_transformers import SentenceTransformer, util
import torch

# --- Masquer le menu Streamlit ---
hide_default_menu = """
    <style>
    [data-testid="stSidebarNav"] {
        display: none;
    }
    </style>
"""
st.markdown(hide_default_menu, unsafe_allow_html=True)
st.set_page_config(page_title="Chatbot IA - IAMBA", page_icon="ü§ñ")
st.title("ü§ñ Agent IA - IAMBA")

# --- API Hugging Face ---
API_URL = "https://api-inference.huggingface.co/models/deepset/roberta-base-squad2"
headers = {"Authorization": "Bearer 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"}

def question_reponse(question, contexte):
    payload = {"inputs": {"question": question, "context": contexte}}
    try:
        response = requests.post(API_URL, headers=headers, json=payload, timeout=10)
        data = response.json()
        if isinstance(data, list) and len(data) > 0 and "answer" in data[0]:
            return data[0]["answer"]
        else:
            return "Je n‚Äôai pas trouv√© d‚Äôinformation pr√©cise dans le contexte fourni."
    except:
        return "D√©sol√©, une erreur est survenue avec l'API Hugging Face."

# --- Embeddings pour contexte ---
embedder = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')

def charger_profil(fichier):
    with open(fichier, 'r', encoding='utf-8') as f:
        texte = f.read()
    blocs = [bloc.strip() for bloc in texte.split("\n\n") if bloc.strip()]
    return blocs

def bloc_pertinent(question, blocs):
    embeddings_blocs = embedder.encode(blocs, convert_to_tensor=True)
    embedding_question = embedder.encode(question, convert_to_tensor=True)
    scores = util.cos_sim(embedding_question, embeddings_blocs)[0]
    top_indices = torch.topk(scores, k=3).indices
    return " ".join([blocs[i] for i in top_indices])

# --- R√©ponses manuelles enrichies ---
def reponse_personnalisee(question):
    q = question.lower()
    if any(kw in q for kw in ["bonjour", "salut", "hello", "bonsoir"]):
        return "Bonjour et bienvenue ! Je suis IAMBA, l‚Äôagent IA de Yamba. Pose-moi toutes tes questions üòä"
    elif any(kw in q for kw in ["merci", "thanks"]):
        return "Avec plaisir ! Je suis toujours disponible."
    elif any(kw in q for kw in ["qui es-tu", "pr√©sente-toi", "tu es qui", "c'est quoi iamba"]):
        return "Je suis IAMBA, l‚Äôagent conversationnel de Yamba Ars√®ne Goubgou, √©conomiste et data scientist bas√© √† Nantes."
    elif any(kw in q for kw in ["√ßa va", "tu vas bien", "comment tu vas"]):
        return "Je vais tr√®s bien, merci ! Et toi ? üòä"
    elif any(kw in q for kw in ["contact", "joindre", "email", "t√©l√©phone"]):
        return ("Tu peux joindre Yamba √† **lamoussama225@gmail.com**, au **+33 7 53 60 73 32** ou via GitHub : [github.com/ygoubgou](https://github.com/ygoubgou)")
    elif "exp√©rience" in q:
        return ("Yamba travaille comme Data Engineer / Analyst chez Cerfrance √† Nantes, o√π il d√©veloppe des outils de pr√©vision "
                "et de reporting avec Python, Power BI et Excel. Il a aussi encadr√© des √©tudiants en √©conom√©trie √† l‚ÄôUniversit√© d‚ÄôAngers.")
    elif "formation" in q:
        return ("Yamba est dipl√¥m√© en √©conomie appliqu√©e (Burkina Faso), en ing√©nierie des donn√©es (Universit√© d‚ÄôAngers), "
                "et suit actuellement un Mast√®re Sp√©cialis√© MQDE √† l‚ÄôENSAE Paris.")
    elif "comp√©tences" in q:
        return ("Il ma√Ætrise Python, R, Stata, SAS, Power BI, Git, et des m√©thodes avanc√©es en √©conom√©trie, machine learning et data visualisation.")
    elif "localisation" in q or "disponibilit√©" in q:
        return "Il est bas√© √† Nantes, disponible en t√©l√©travail ou en pr√©sentiel, et titulaire du permis B."
    return None

# --- Chargement du profil ---
blocs_profil = charger_profil("profil.txt")
question = st.text_input("Pose ta question √† IAMBA (ex: Formation, Comp√©tences, Exp√©riences...)")

if question:
    reponse_man = reponse_personnalisee(question)
    if reponse_man:
        st.markdown("**üß† R√©ponse personnalis√©e :**")
        st.success(reponse_man)
    else:
        contexte = bloc_pertinent(question, blocs_profil)
        reponse = question_reponse(question, contexte)
        st.markdown("**üìÑ Contexte utilis√© :**")
        st.info(contexte)
        st.markdown("**üß† R√©ponse API :**")
        st.success(reponse)
